# Sprint 8 (AWS - DESAFIO FINAL) - Trilha de Data Engineering

### Introdu√ß√£o

Nesta oitava sprint da trilha de Data Engineering, avan√ßamos no desafio final com foco no processamento de dados e na pr√°tica de manipula√ß√£o de dados em larga escala. Realizamos exerc√≠cios que consolidaram conhecimentos de manipula√ß√£o de listas, gera√ß√£o de datasets em Python e uso do Apache Spark para an√°lises avan√ßadas. Essas pr√°ticas complementaram o desafio principal, que envolveu a transforma√ß√£o de dados na camada Trusted do Data Lake usando AWS Glue e Spark.

## Aprendizados

### **Manipula√ß√£o de Dados em Python**
- **Listas e Arquivos**:
  - Criamos uma lista com 250 inteiros aleat√≥rios, invertendo sua ordem.
  - Organizamos nomes de animais em ordem alfab√©tica e os salvamos em um arquivo CSV.
  - Geramos um dataset de nomes aleat√≥rios utilizando a biblioteca `names`, salvando-o em um arquivo de texto.

### **An√°lise de Dados com Apache Spark**
- Criamos um DataFrame a partir de um arquivo contendo aproximadamente 10 milh√µes de nomes.
- Realizamos opera√ß√µes de transforma√ß√£o e filtragem, como:
  - Adicionar colunas de atributos gerados aleatoriamente, como "Escolaridade", "Pa√≠s" e "Ano de Nascimento".
  - Filtrar dados para identificar pessoas de gera√ß√µes espec√≠ficas.
  - Executar an√°lises SQL para agrupar e ordenar informa√ß√µes por pa√≠s e gera√ß√£o.

### **Processamento de Dados na Trusted Zone**
- Desenvolvemos dois jobs no AWS Glue para transformar e organizar os dados provenientes da camada Raw:
  - Processamento de arquivos CSV e JSON com persist√™ncia no formato PARQUET.
  - Organiza√ß√£o e particionamento dos dados JSON por ano, m√™s e dia.

### **Documenta√ß√£o e Evid√™ncias**
- Documentamos cada etapa do desafio, com logs, prints e explica√ß√µes claras.
- Consolidamos pr√°ticas de organiza√ß√£o para facilitar futuras reprodu√ßes do processo.

## Desafios

### **Entrega 3: Processamento Camada Trusted**
- **Job 1: Processamento de arquivos CSV**:
  - Transforma√ß√£o e persist√™ncia no S3 no formato PARQUET, sem particionamento.
- **Job 2: Processamento de arquivos JSON**:
  - Aplica√ß√£o de particionamento por data no S3, seguindo boas pr√°ticas de organiza√ß√£o.

### Configura√ß√£o dos Jobs:
- **Worker type:** G 1x.
- **Requested number of workers:** 2.
- **Job timeout:** 60 minutos ou menos.

confira aqui o desafio da sprint 8:

- [üìÅDesafio](../sprint_8/desafio)


## Exerc√≠cios

- **Exerc√≠cio 1: Manipula√ß√£o de Dados em Python**:
  - Cria√ß√£o de listas, ordena√ß√£o de nomes e gera√ß√£o de datasets.
  - Persist√™ncia de dados em arquivos CSV e TXT.
  

  ### Etapas:
  1. **Etapa 1**: Em Python, declare e inicialize uma lista contendo 250 inteiros obtidos de forma aleat√≥ria. Ap√≥s, aplicar o m√©todo reverse sobre o conte√∫do da lista e imprimir o resultado.

    - [üìÅC√≥digo Etapa 1](../sprint_8/exercicios/exec_1/etapa_1.py)

  2. **Etapa 2**: Em Python, declare e inicialize uma lista contendo o nome de 20 animais. Ordene-os em ordem crescente e itere sobre os itens, imprimindo um a um. Armazene o conte√∫do da lista em um arquivo de texto, um item em cada linha, no formato CSV.
    - [üìÅC√≥digo Etapa 2](../sprint_8/exercicios/exec_1/etapa_2.py)

    - [üìÅ arquivo animais.csv gerado](../sprint_8/exercicios/exec_1/animais.csv)

  3. **Etapa 3**: Elaborar um c√≥digo Python para gerar um dataset de nomes de pessoas.

  OBS: o arquivo nomes_aleatorios.txt gerado est√° com incluido no .gitignore, para evitar sobrecarga no reposit√≥rio, devido ao tamanho do arquivo.

    - [üìÅC√≥digo Etapa 3](../sprint_8/exercicios/exec_1/etapa_3.py)

- # **Exerc√≠cio 2: An√°lise de Dados com Apache Spark**:
  - Cria√ß√£o e transforma√ß√£o de DataFrames.
  - Uso de comandos SQL para realizar an√°lises avan√ßadas.

  - [üìÅExercicio-2_Spark_DataFrames](../sprint_8/exercicios/exec_2)

  - [üìÅC√≥digo Exerc√≠cio 2](../sprint_8/exercicios/exec_2/exercicio_2.ipynb)




## Certificados 
Nesta sprint n√£o houveram cursos complementares, logo, n√£o foi necess√°ria a comprova√ß√£o por meio de certificados.



## Conclus√£o

A Sprint 8 foi essencial para consolidar habilidades pr√°ticas em manipula√ß√£o de dados, processamento com Apache Spark e uso do AWS Glue. Essas atividades complementaram o desafio principal, fortalecendo a compreens√£o das etapas necess√°rias para integrar e transformar dados em um ambiente de Data Lake. Seguimos bem preparados para novos desafios e aplica√ß√µes do que aprendemos.
